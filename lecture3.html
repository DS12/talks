<!DOCTYPE html>
<!--
  Copyright 2010 Google Inc.

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.

  Original slides: Marcin Wichary (mwichary@google.com)
  Modifications: Ernest Delgado (ernestd@google.com)
                 Alex Russell (slightlyoff@chromium.org)

  landslide modifications: Adam Zapletal (adamzap@gmail.com)
                           Nicolas Perriault (nperriault@gmail.com)
-->
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Lecture 3: Matrix Decompositions</title>
    <!-- Styles -->
    
    <link rel="stylesheet" media="print" href="theme/css/print.css">
    <link rel="stylesheet" media="screen, projection" href="theme/css/screen.css">
    
    
    <!-- /Styles -->
    <!-- Javascripts -->
    
    <script type="text/javascript" src="theme/js/slides.js"></script>
    
    
    <!-- /Javascripts -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body style="!important;">
  <div id="blank"></div>
  <div class="presentation">
    <div id="current_presenter_notes">
      <div id="presenter_note"></div>
    </div>
    <div class="slides">
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Lecture 3: Matrix Decompositions</h1></header>
            
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              1/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Emmy Noether</h1></header>
            
            
            <section><p><img alt="" src="img/noether.png" />
<br>
<br></p>
<p>"My methods are really methods of working and thinking; this is why they have crept in everywhere anonymously."</p>
<ul>
<li>1931 letter to Helmut Hasse </li>
</ul></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              2/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Notation</h1></header>
            
            
            <section><p>In this course we will notate vectors $x,y,z \in \mathbb{R}^m$ with no special identifiers ('vector' here means a $m \times 1$ column vector). Scalars will be identified by letters in the Greek alphabet (e.g. $\lambda, \mu, \sigma$), and the beginning of the English alphabet (e.g. $a, b, c$).
$$
$$
Matrices $A \in \mathbb{R}^{n \times m}$, which represent linear transformations from $\mathbb{R}^m \to \mathbb{R}^n$ with respect to some basis, will always be denoted with capital (English or Greek) letters. </p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              3/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>Outer and inner products between vectors will be written using matrix notation, so $xx^T$ is a $m \times m$ matrix of rank 1 and $x^Tx$ is a $1 \times 1$ matrix whose value is $\|x\|^2$.
$$
$$
Given two matrices $A$  and $B$, their product can be computed as a sum of either inner or outer products of the rows of $A$ and columns of $B$, or columns of $A$ and rows of $B$ respectively. 
<br>
<br>
The resulting product $AB$ is the same, however the choice of method will often have computational ramifications.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              4/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Exercise</h1></header>
            
            
            <section><p>Compute $A^2$ using both inner and outer product methods.</p>
<p>$$
A = 
\begin{pmatrix}
1 &amp; 2 \\
3 &amp; 4 \\
\end{pmatrix} 
$$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              5/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Positive Definite Matrices</h1></header>
            
            
            <section><p>A matrix $A$ is symmetric if $A = A^T$. A symmetric matrix $A$ is positive definite (often written $A \geq 0$) if the scalar $x^TAx \geq 0$ for all $x \neq 0$.
$$
$$
Positive definite matrices are closely related to positive-definite symmetric bilinear forms, and to inner products of vector spaces.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              6/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Properties</h1></header>
            
            
            <section><p>Let $A \in \mathbb{R}^{n \times n}$ be a symmetric matrix. $A \geq 0$ if and only if:</p>
<ul>
<li>$x^TAx \geq 0$ for all $x \neq 0$</li>
<li>The eigenvalues of $A$ are all positive.</li>
<li>$A$ is the <a href="https://en.wikipedia.org/wiki/Gramian_matrix">Gramian matrix</a> matrix of $n$ linearly independent vectors: $A = X^TX$ (see also Mercer's Theorem).</li>
<li>$A$ has a unique Cholesky decomposition: $ A = L L^T$, where L is a lower triangular matrix with real and positive diagonal entries.</li>
</ul></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              7/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Exercise</h1></header>
            
            
            <section><p>Show that
$$
A = \begin{pmatrix} 2&amp;-1&amp;0 \\ -1&amp;2&amp;-1 \\ 0&amp;-1&amp;2 \end{pmatrix} 
$$
is positive definite.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              8/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Application: Covariance Matrices</h1></header>
            
            
            <section><p>Given $N$ data vectors $x_i \in \mathbb{R}^d$, the sample covariance matrix is a $d \times d$ matrix $A$:</p>
<p>$$
A = {1 \over {N}}\sum_{i=1}^N (x_i-\bar{x}) (x_i-\bar{x})^\mathrm{T}
$$</p>
<p>If the $x_i$ are mean-centered and considered as rows in a data matrix $X$, then the sample covariance is simply a scalar multiple of $X^TX$, the Gramian of $X$.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              9/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Spectral Theorem</h1></header>
            
            
            <section><p><strong>Theorem:</strong>  Let $A: \mathcal{V} \to \mathcal{V}$ be symmetric (Hermitian). There exists an orthonormal basis of $\mathcal{V}$ consisting of eigenvectors $e_i$ of $A$. Each eigenvalue $\sigma_i$ is real. In other words
$$
A = E \Sigma E^T
$$
where $E$ is orthogonal ($E^T = E^{-1}$) and $\Sigma$ is a diagonal matrix of real eigenvalues $\sigma_i$.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              10/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide has_notes">
          <div class="inner">
            
            <header><h1>Application: PCA</h1></header>
            
            
            <section><p>We can apply the spectral theorem to any symmetric matrix, in particular we can apply it to Gramian matrices $X^TX$ coming from sample covariances. 
$$
$$
We call this the Principal Components Analysis of $X$. </p>
<p class="notes">PCA is an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some projection of the data comes to lie on the first coordinate, the second greatest variance on the second coordinate, and so on.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              11/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>A full set of real-valued eigenvalues $\sigma_i$ of $X^TX$ exist (by the Spectral Theorem), furthermore they are positive (by positive definiteness). 
$$
$$
In the context of PCA $e_i$ is the i-th principal component and $\sigma_i$ is the sample variance of the dataset in the direction of $e_i$.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              12/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>PCA can be thought of as fitting a multivariate normal distribution $\mathcal{N}(\mu, \Sigma)$ to the data by maximum liklihood. 
$$
$$
Each axis of the ellipsoid defined by $\hat{\Sigma} = X^TX $ represents a principal component. 
$$
$$
If some axis of the ellipse is small, then the variance along that axis is also small, and by omitting that axis and its corresponding principal component from our representation of the dataset, we lose only a commensurately small amount of information.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              13/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>To find the axes of the ellipse, we mean-center the data and compute the covariance matrix $X^TX$, then calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. 
$$
$$
Finally, we <a href="https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process">normalize</a> the eigenvectors and sort according to eigenvalue: $\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_n$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              14/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Exercise</h1></header>
            
            
            <section><p>Compute the PCA of the following data matrix $X$:</p>
<p>$$
X =
\begin{pmatrix}
1 &amp; 2 \\
2 &amp; 1 \\
3 &amp; 4 \\
4 &amp; 3 \\
\end{pmatrix} 
$$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              15/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>For large matrices, working directly with the eigenvalue problem $X^TX$ is numerically unstable in finite precision arithmetic.
$$
$$
We will therefore explore two alternate solution methods, the latter of which will be useful for our explorations of latent variable models later in the course.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              16/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>One can also frame the principal component vectors $e_i$ as solutions to a series of optimization problems involving <a href="https://en.wikipedia.org/wiki/Rayleigh_quotient">Rayleigh quotients</a>:
\begin{align}
e_1 &amp;= \max_{\|v\|=1} \|X v\|^2 = \max_{\|v\|=1} v^T X^TX v =  \max \frac{v^T X^TX v}{\|v\|}
\newline
e_2 &amp;= \max_{v \in \mathcal{Im}(e_1)^\perp} \frac{v^T X^TX v}{\|v\|}
\newline
e_3 &amp;= \max_{v \in \mathcal{Im}(e_1,e_2)^\perp} \frac{v^T X^TX v}{\|v\|}
\newline
\dots
\end{align}</p>
<p>Where the space $\mathcal{Im}(e_1)^\perp$ is the orthogonal complement of $\mathcal{Im}(e_1)$.
$$
$$
This is essentially a rephrasing of the <a href="https://en.wikipedia.org/wiki/Courant_minimax_principle">Courant minimax principle</a>.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              17/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>SVD</h1></header>
            
            
            <section><p>The Singular Value Decomposition is a useful generalization of the spectral theorem on positive definite matrices to rectangular matrices.
$$
$$
<strong>Theorem:</strong>  Let $A :\mathcal{X} \to \mathcal{Y}$ be a linear map between finite dimensional inner product spaces. There is an orthonormal basis $\{v_1, \cdots, v_m\}$ for $\mathcal{X}$ such that $ (Av_i)^TAv_j = 0$ if $i \neq j$. 
<br>
<br>
Furthermore, we can find an orthonormal basis for $\mathcal{Y}$, and non-negative values $\sigma_1, \cdots, \sigma_r$ such that:</p>
<p>\begin{align}
Av_1 = \sigma_1 u_1, \cdots, Av_r &amp;= \sigma_r u_r,
\newline
Av_{r+1}  = 0, \cdots,  Av_m &amp;= 0       <br />
\end{align}
for some $r \leq m$. In matrix form we have:
$$
A = \sum_i \sigma_i u_i v_i^T = U \Sigma V^T<br />
$$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              18/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>We sketch a proof as follows: use the spectral theorem on $A^TA$ to find an orthonormal basis $\{v_1, \cdots, v_m\}$ for $\mathcal{X}$ such that $A^TAv_i = \lambda v_i$. This gives the first part, since
$$
(Av_i)^TAv_j = v_i^T A^T A v_j = v_i^T \lambda_j v_j = \lambda_j \delta_{ij}
$$
where $\delta_{ij}$ is the Dirac delta function.
<br>
<br>
Next we reorder the basis if necessary so that the $r$ non-zero eigenvectors come first. We then define
$$
u_i = \frac{Av_i}{\|Av_i \|}
$$
for $i = 1, \cdots, k$. Finally select $u_{k+1}, \cdots, u_n$ to complete the orthonomal basis for $\mathcal{Y}$. 
<br>
<br>
The values $\sigma_i = \sqrt{\lambda_i}$ are known as the singular values of $ A $.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              19/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Example</h1></header>
            
            
            <section><p>The following is an SVD for arbitrary values of $\theta \in \mathbb{R}$:
<br>
<br></p>
<p>$$
\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; -1 \end{pmatrix} = 
\begin{pmatrix} \cos(\theta) &amp; \sin(\theta) \\ - \sin(\theta) &amp; \cos(\theta) \end{pmatrix} 
\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix} 
\begin{pmatrix} \cos(\theta) &amp; \sin(\theta) \\  \sin(\theta) &amp; - \cos(\theta) \end{pmatrix} 
$$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              20/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Example</h1></header>
            
            
            <section><p>The following is an SVD 
<br>
<br>
$$
\begin{pmatrix} 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 \\ 3 &amp; 3 &amp; 3 &amp; 0 &amp; 0 \\ 4 &amp; 4 &amp; 4 &amp; 0 &amp; 0 \\ 5 &amp; 5 &amp; 5 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 4 &amp; 4 \\  0 &amp; 0 &amp; 0 &amp; 5 &amp; 5 \\  0 &amp; 0 &amp; 0 &amp; 2 &amp; 2 \end{pmatrix} = 
\begin{pmatrix} .14 &amp; 0 \\ .42 &amp; 0 \\ .56 &amp; 0 \\ .70 &amp; 0 \\ 0 &amp; .60 \\ 0 &amp; .75 \\ 0 &amp; .30  \end{pmatrix} 
\begin{pmatrix} 12.4 &amp; 0 \\ 0 &amp; 9.5 \end{pmatrix} 
\begin{pmatrix} .58 &amp; .58 &amp; .58 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; .71 &amp; .71 \end{pmatrix} 
$$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              21/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Example</h1></header>
            
            
            <section><p>The SVD of the matrix $X$ from above is:
$$
\begin{pmatrix}
1 &amp; 2 \\
2 &amp; 1 \\
3 &amp; 4 \\
4 &amp; 3 \\
\end{pmatrix} 
=
\begin{pmatrix}
\frac{3}{\sqrt{116}} &amp; \frac{1}{2} \\
\frac{3}{\sqrt{116}} &amp; -\frac{1}{2} \\
\frac{7}{\sqrt{116}} &amp; \frac{1}{2} \\
\frac{7}{\sqrt{116}} &amp; -\frac{1}{2} \\
\end{pmatrix} 
\begin{pmatrix}
\sqrt{58} &amp; 0 \\
0 &amp; \sqrt{2} \\
\end{pmatrix} 
\begin{pmatrix}
\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\
-\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}}  \\
\end{pmatrix} 
$$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              22/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>The proof of the SVD decomposition theorem suggests that the SVD of a matrix $X$ is closely related to the eigenvectors of $X^TX$. Working backwards we have:</p>
<p>\begin{align}
X^TX &amp;= (U \Sigma V^T )^T U \Sigma V^T<br />
\newline
&amp;= V \Sigma^T U^T U  \Sigma    V^T 
\newline
&amp;=  V  \Sigma^2  V^T    <br />
\end{align}</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              23/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>The full set of $T$ transformed coordinates from the PCA of $X$ can then be written as
\begin{align}
T &amp; = X V \\
&amp; = U\Sigma V^T V \\
&amp; = U\Sigma
\end{align}
so each column of $T$ is given by one of the left singular vectors of $X$ multiplied by the corresponding singular value. 
<br>
<br>
Computing the SVD is the standard way to calculate the PCA of a data matrix.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              24/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>The SVD can also used to compute a rank-$k$ approximation of $X$:
$$
$$
<strong>Proposition:</strong> The rank-$k$ SVD approximation of $X$ is equivalent to a reconstruction of $X$ from its first $k$ principal components.</p>
<p>Let $U \Sigma_k V^T$ be a truncated SVD of $X$. We know $Z = X V$, so</p>
<p>$$
Z_k = U \Sigma_k V^T V = U \Sigma_k 
$$<br />
The reconstruction of $X$ from its first $k$ principal components is given by $X_k = Z_k V^T$ so
$$
X_k = Z_k V^T = U \Sigma_k V^T
$$
as above.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              25/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Application: Image Compression</h1></header>
            
            
            <section><p>$A_k$ requires $\mathcal{O}(nk + k + km)$ storage. So for example using a rank-20 approximation to a $1000 \times 800$ image would result in a compression ratio of:
$$
\frac{800,000}{1,000 \cdot 20 + 20 + 20 \cdot 800 } \sim 22.21
$$
which is <a href="http://blog.codinghorror.com/a-comparison-of-jpeg-compression-levels-and-recompression/">competitive</a> with the jpg file format.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              26/27
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture3.md -->
      <div class="slide-wrapper">
        <div class="slide has_notes">
          <div class="inner">
            
            
            <section><p><img alt="" src="img/svd2.jpg" />
<img alt="" src="img/svd.jpg" />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="http://cg.skeelogy.com/svd-image-compression/">source</a></p>
<p>However an equivalently-sized jpg file will generally look better because the jpg compression algorithm uses a basis in which most image signals are sparse.</p>
<p class="notes">Of course it doesnt look nearly as good, b/c the Frobenius norm is not how the human eye works. for better results we need sparse dictionaries.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture3.md">lecture3.md</a>
            </aside>
            
            <aside class="page_number">
              27/27
            </aside>

          </footer>
        </div>
      </div>
      
    </div>
  </div>
  
  <div id="toc" class="sidebar hidden">
    <h2>Table of Contents</h2>
    <table>
      <caption>Table of Contents</caption>
      
      <tr id="toc-row-1">
        <th><a href="#slide1">Lecture 3: Matrix Decompositions</a></th>
        <td><a href="#slide1">1</a></td>
      </tr>
      
      
      <tr id="toc-row-2">
        <th><a href="#slide2">Emmy Noether</a></th>
        <td><a href="#slide2">2</a></td>
      </tr>
      
      
      <tr id="toc-row-3">
        <th><a href="#slide3">Notation</a></th>
        <td><a href="#slide3">3</a></td>
      </tr>
      
      
      <tr id="toc-row-4">
        <th><a href="#slide4">-</a></th>
        <td><a href="#slide4">4</a></td>
      </tr>
      
      
      <tr id="toc-row-5">
        <th><a href="#slide5">Exercise</a></th>
        <td><a href="#slide5">5</a></td>
      </tr>
      
      
      <tr id="toc-row-6">
        <th><a href="#slide6">Positive Definite Matrices</a></th>
        <td><a href="#slide6">6</a></td>
      </tr>
      
      
      <tr id="toc-row-7">
        <th><a href="#slide7">Properties</a></th>
        <td><a href="#slide7">7</a></td>
      </tr>
      
      
      <tr id="toc-row-8">
        <th><a href="#slide8">Exercise</a></th>
        <td><a href="#slide8">8</a></td>
      </tr>
      
      
      <tr id="toc-row-9">
        <th><a href="#slide9">Application: Covariance Matrices</a></th>
        <td><a href="#slide9">9</a></td>
      </tr>
      
      
      <tr id="toc-row-10">
        <th><a href="#slide10">Spectral Theorem</a></th>
        <td><a href="#slide10">10</a></td>
      </tr>
      
      
      <tr id="toc-row-11">
        <th><a href="#slide11">Application: PCA</a></th>
        <td><a href="#slide11">11</a></td>
      </tr>
      
      
      <tr id="toc-row-12">
        <th><a href="#slide12">-</a></th>
        <td><a href="#slide12">12</a></td>
      </tr>
      
      
      <tr id="toc-row-13">
        <th><a href="#slide13">-</a></th>
        <td><a href="#slide13">13</a></td>
      </tr>
      
      
      <tr id="toc-row-14">
        <th><a href="#slide14">-</a></th>
        <td><a href="#slide14">14</a></td>
      </tr>
      
      
      <tr id="toc-row-15">
        <th><a href="#slide15">Exercise</a></th>
        <td><a href="#slide15">15</a></td>
      </tr>
      
      
      <tr id="toc-row-16">
        <th><a href="#slide16">-</a></th>
        <td><a href="#slide16">16</a></td>
      </tr>
      
      
      <tr id="toc-row-17">
        <th><a href="#slide17">-</a></th>
        <td><a href="#slide17">17</a></td>
      </tr>
      
      
      <tr id="toc-row-18">
        <th><a href="#slide18">SVD</a></th>
        <td><a href="#slide18">18</a></td>
      </tr>
      
      
      <tr id="toc-row-19">
        <th><a href="#slide19">-</a></th>
        <td><a href="#slide19">19</a></td>
      </tr>
      
      
      <tr id="toc-row-20">
        <th><a href="#slide20">Example</a></th>
        <td><a href="#slide20">20</a></td>
      </tr>
      
      
      <tr id="toc-row-21">
        <th><a href="#slide21">Example</a></th>
        <td><a href="#slide21">21</a></td>
      </tr>
      
      
      <tr id="toc-row-22">
        <th><a href="#slide22">Example</a></th>
        <td><a href="#slide22">22</a></td>
      </tr>
      
      
      <tr id="toc-row-23">
        <th><a href="#slide23">-</a></th>
        <td><a href="#slide23">23</a></td>
      </tr>
      
      
      <tr id="toc-row-24">
        <th><a href="#slide24">-</a></th>
        <td><a href="#slide24">24</a></td>
      </tr>
      
      
      <tr id="toc-row-25">
        <th><a href="#slide25">-</a></th>
        <td><a href="#slide25">25</a></td>
      </tr>
      
      
      <tr id="toc-row-26">
        <th><a href="#slide26">Application: Image Compression</a></th>
        <td><a href="#slide26">26</a></td>
      </tr>
      
      
      <tr id="toc-row-27">
        <th><a href="#slide27">-</a></th>
        <td><a href="#slide27">27</a></td>
      </tr>
      
      
    </table>
  </div>
  
  <div id="help" class="sidebar hidden">
    <h2>Help</h2>
    <table>
      <caption>Help</caption>
      <tr>
        <th>Table of Contents</th>
        <td>t</td>
      </tr>
      <tr>
        <th>Expos√©</th>
        <td>ESC</td>
      </tr>
      <tr>
        <th>Full screen slides</th>
        <td>e</td>
      </tr>
      <tr>
        <th>Presenter View</th>
        <td>p</td>
      </tr>
      <tr>
        <th>Source Files</th>
        <td>s</td>
      </tr>
      <tr>
        <th>Slide Numbers</th>
        <td>n</td>
      </tr>
      <tr>
        <th>Toggle screen blanking</th>
        <td>b</td>
      </tr>
      <tr>
        <th>Show/hide slide context</th>
        <td>c</td>
      </tr>
      <tr>
        <th>Notes</th>
        <td>2</td>
      </tr>
      <tr>
        <th>Help</th>
        <td>h</td>
      </tr>
    </table>
  </div>
  <script>main()</script>
</body>
</html>