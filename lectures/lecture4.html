<!DOCTYPE html>
<!--
  Copyright 2010 Google Inc.

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.

  Original slides: Marcin Wichary (mwichary@google.com)
  Modifications: Ernest Delgado (ernestd@google.com)
                 Alex Russell (slightlyoff@chromium.org)

  landslide modifications: Adam Zapletal (adamzap@gmail.com)
                           Nicolas Perriault (nperriault@gmail.com)
-->
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Lecture 4: Linear Regression</title>
    <!-- Styles -->
    
    <link rel="stylesheet" media="print" href="theme/css/print.css">
    <link rel="stylesheet" media="screen, projection" href="theme/css/screen.css">
    
    
    <!-- /Styles -->
    <!-- Javascripts -->
    
    <script type="text/javascript" src="theme/js/slides.js"></script>
    
    
    <!-- /Javascripts -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>
<body style="!important;">
  <div id="blank"></div>
  <div class="presentation">
    <div id="current_presenter_notes">
      <div id="presenter_note"></div>
    </div>
    <div class="slides">
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Lecture 4: Linear Regression</h1></header>
            
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              1/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p><img alt="" src="img/gauss.jpg" />
<br>
<br>
Linear regression, famously used by Gauss in 1809 to <a href="https://www.math.rutgers.edu/\simcherlin/History/Papers1999/weiss.html">predict the location of Ceres</a>, is the ur-model of supervised learning.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              2/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Problem</h1></header>
            
            
            <section><p>Suppose $A \in \mathbb{R}^{m \times n}$ with $m \neq n$ and $y \in \mathbb{R}^m$ is a given vector. Find $x \in \mathbb{R}^n$ such that $y = Ax$.
$$
$$
However if no $x$ satisfies the equation or more than one $x$ does -- that is the solution is not unique -- the problem is said not to be well posed. </p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              3/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>The standard approach in the (overdetermined) case where $m &gt; n$ is linear least squares linear regression. 
$$
$$
In this situation the linear system $Ax = y$ is not solvable for most values of $y$.
$$
$$
The <em>linear least squares problem</em> poses an approximate solution in the following constrained optimization problem:</p>
<p>$$
\min_{b \in \mathcal{Im}(A)} \|y - b\|^2
$$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              4/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Pseudoinverses</h1></header>
            
            
            <section><p>The SVD gives us a nice way of studying linear systems $Ax = y$ where the matrix is not invertible.
$$
$$
In this case $A$ has a generalized inverse called the Moore-Penrose psuedoinverse (denoted $A^+$).
$$
$$
The Moore-Penrose psuedoinverse is defined for any real-valued matrix $A$, and corresponds to the normal inverse $A^{-1}$ when $A$ is invertible.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              5/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>We write $\oplus$ for the direct sum of inner product spaces, $\perp$ for the orthogonal complement, $\mathcal{Ker}$ for the kernel of a map, and $\mathcal{Im}$  for the image of a map.
$$
$$
If we view the matrix as a linear map $A :\mathcal{X} \to \mathcal{Y}$ then $A$ decomposes the two spaces as follows:</p>
<p>\begin{align}
\mathcal{X} &amp;= \mathcal{Ker}(A )^\perp \oplus\mathcal{Ker}(A )
\newline
\mathcal{Y} &amp;= \mathcal{Im}(A )^\perp \oplus\mathcal{Im}(A )
\end{align}</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              6/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>The restriction  $A : \mathcal{Ker}(A )^\perp \to  \mathcal{Im}(A)$ is then an isomorphism. 
$$
$$
$A^+$ is defined on $ \mathcal{Im}(A)$ to be the inverse of this isomorphism, and on $ \mathcal{Im}(A)^\perp$ to be zero.
$$
$$
Therefore for $y \in \mathcal{Im}(A)$, $A^+y$ is the unique vector $x$ such that $y = A(x)$ and $x \in \mathcal{Ker}(A)^\perp$.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              7/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>This construction gives us the following nice properties for $A^+$:</p>
<p>\begin{align}
\mathcal{Ker}(A^+) &amp;= \mathcal{Im}(A)^\perp<br />
\newline
\mathcal{Im}(A^+)  &amp; = \mathcal{Ker}(A)^\perp          <br />
\end{align}</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              8/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Exercise</h1></header>
            
            
            <section><p>What is the pseudoinverse of $X$?</p>
<p>$$
X =
\begin{pmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 0 \\
\end{pmatrix} 
$$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              9/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Properties</h1></header>
            
            
            <section><ul>
<li>If $A$ has real entries, then so does $ A^+$.</li>
<li>If $A$ is invertible, its pseudoinverse is its inverse.  That is: $A^+=A^{-1}$.</li>
<li>The pseudoinverse of the pseudoinverse is the original matrix: $(A^+)^+=A$.</li>
<li>Pseudoinversion commutes with transposition:$(A^T)^+ = (A^+)^T$.</li>
<li>The pseudoinverse of a scalar multiple of $A$ is the reciprocal multiple of $A$: $(\alpha A)^+ = \alpha^{-1} A^+$ for $\alpha\neq 0$.</li>
</ul></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              10/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide has_notes">
          <div class="inner">
            
            
            <section><p><strong>Theorem (Analytic Characterization):</strong>  Let $A \in \mathbb{R}^{n \times m}$. Then 
$$
A^+ = \lim_{\lambda \to 0} (A^TA  + \lambda I)^{-1} A^T<br />
$$</p>
<p>This characterization (known as <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization">Tikhonov regularization</a>) leads to an iterative method for computing $A^+$ (see Ben-Israel and Cohen).</p>
<p class="notes">note the connection to ridge regression.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              11/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Exercise</h1></header>
            
            
            <section><p>Find the pseudoinverse of $A$ by checking the conditions of the above theorem.</p>
<p>$$
A = 
\begin{pmatrix}
1 &amp; 1 \\
1 &amp; 1 \\
\end{pmatrix} 
$$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              12/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source:  -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="page_number">
              /38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>\begin{split}
A^+ &amp;= \lim_{\lambda \to 0} (A^TA  + \lambda I)^{-1} A^T<br />
\newline
&amp;= \lim_{\lambda \to 0} 
\begin{pmatrix}
2+\lambda &amp; 2 \\
2 &amp; 2+\lambda \\
\end{pmatrix} ^ {-1}
\begin{pmatrix}
1 &amp; 1 \\
1 &amp; 1 \\
\end{pmatrix} 
\newline
&amp;= \lim_{\lambda \to 0}  \frac{1}{\lambda(\lambda+4)}
\begin{pmatrix}
2+ \lambda &amp; -2 \\
-2 &amp; 2 + \lambda \\
\end{pmatrix} 
\begin{pmatrix}
1 &amp; 1 \\
1 &amp; 1 \\
\end{pmatrix}
\newline
&amp;= \lim_{\lambda \to 0}  \frac{1}{\lambda(\lambda+4)}
\begin{pmatrix}
\lambda &amp; \lambda \\
\lambda &amp; \lambda \\
\end{pmatrix} 
\newline
&amp;= \lim_{\lambda \to 0}  \frac{1}{\lambda+4}
\begin{pmatrix}
1&amp; 1 \\
1 &amp; 1\\
\end{pmatrix} 
\newline
&amp;=  \frac{1}{4}
\begin{pmatrix}
1 &amp; 1 \\
1 &amp; 1\\
\end{pmatrix} 
\end{split}</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              13/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>We can also define $A^+$ for all $A \in \mathbb{R}^{n \times m}$ according to purely algebraic criteria.
$$
$$
<strong>Theorem (Algebraic Characterization):</strong> Let $A \in \mathbb{R}^{m \times n}$. Then $G = A^+$ if and only if:</p>
<ul>
<li>$A G A  = A $</li>
<li>$G A G  = G $</li>
<li>$(A G)^T  = AG  $</li>
<li>$(G A)^T  = GA  $</li>
</ul>
<p>Furthermore, $ A^+$ always exists and is unique. Note also that when an true $A^{-1}$ exists it satisfies the above properties.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              14/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide has_notes">
          <div class="inner">
            
            
            <section><p>A conceptually simpler way to compute the pseudoinverse is by using the SVD of $A$.
$$
$$
If $A = U\Sigma V^T$ is the singular value decomposition of $A$ then $A^+ = V\Sigma^+ U^T$. 
$$
$$
For a rectangular diagonal matrix such as $\Sigma$, we can easily get the pseudoinverse by taking the reciprocal of each non-zero element on the diagonal, leaving the zeros in place, and then transposing the matrix, as in the Exercise above. </p>
<p class="notes">In numerical computation, only elements larger than some small tolerance are taken to be nonzero, and the others are replaced by zeros. </p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              15/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Example</h1></header>
            
            
            <section><p>Lets compute psuedoinverse using the SVD of the matrix $X$ from the last lecture.</p>
<p>$$
X =
\begin{pmatrix}
1 &amp; 2 \\
2 &amp; 1 \\
3 &amp; 4 \\
4 &amp; 3 \\
\end{pmatrix} 
$$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              16/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>We saw that the SVD of $X$ is:
$$
\begin{pmatrix}
1 &amp; 2 \\
2 &amp; 1 \\
3 &amp; 4 \\
4 &amp; 3 \\
\end{pmatrix} 
=
\begin{pmatrix}
\frac{3}{\sqrt{116}} &amp; \frac{1}{2} \\
\frac{3}{\sqrt{116}} &amp; -\frac{1}{2} \\
\frac{7}{\sqrt{116}} &amp; \frac{1}{2} \\
\frac{7}{\sqrt{116}} &amp; -\frac{1}{2} \\
\end{pmatrix} 
\begin{pmatrix}
\sqrt{58} &amp; 0 \\
0 &amp; \sqrt{2} \\
\end{pmatrix} 
\begin{pmatrix}
\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\
-\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}}  \\
\end{pmatrix} 
$$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              17/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>Therefore the pseudoinverse $X^+$ is:</p>
<p>\begin{split}
X^+ &amp;=
\begin{pmatrix}
\frac{1}{\sqrt{2}} &amp; -\frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}}  \\
\end{pmatrix} 
\begin{pmatrix}
\frac{1}{\sqrt{58}} &amp; 0 \\
0 &amp; \frac{1}{\sqrt{2}} \\
\end{pmatrix} 
\begin{pmatrix}
\frac{3}{\sqrt{116}} &amp; \frac{3}{\sqrt{116}} &amp; \frac{7}{\sqrt{116}} &amp; \frac{7}{\sqrt{116}}  \\
\frac{1}{2} &amp; -\frac{1}{2} &amp; \frac{1}{2} &amp; -\frac{1}{2} \\
\end{pmatrix}
\newline
&amp;= \frac{1}{58} 
\begin{pmatrix}
-13 &amp; 16 &amp; -11 &amp; 18  \\
16 &amp; -13 &amp; 18 &amp; -11  \\
\end{pmatrix} 
\end{split}</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              18/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>Note that both the SVD and the Moore-Penrose psuedoinverse have poor sparsity properties. 
$$
$$
As such, these methods have limited utility for very large matrices. </p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              19/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>The following special case will be useful in solving the linear least squares problem.
$$
$$
<strong>Proposition:</strong> If $A$ is injective (e.g. $A$ is full rank and $m \geq n$), then  $A^+ = (A^TA)^{-1} A^T $
$$
$$
The proof follows from the algebraic criteria above and the fact that $A^TA$ is a positive definite matrix.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              20/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Example</h1></header>
            
            
            <section><p>For any (column) vector $x \neq 0$:</p>
<p>$$
x^+ = (x^Tx)^+x^T = (x^Tx)^{-1}x^T
$$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              21/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>In the linear least squares context, the system:
$$
\beta = A^+y = (A^TA)^{-1} A^T y
$$
i.e. the Moore-Penrose psuedoinverse of $A$ applied to $y$, is referred to as the <em>normal equations</em>.
$$
$$
The matrix $(A^TA)^{-1} A^T$ itself is sometimes referred to as the 'hat matrix'.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              22/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide has_notes">
          <div class="inner">
            
            <header><h1>Exercise</h1></header>
            
            
            <section><p>Given the following data: $(1,2), (2,1), (3,4), (4,3)$, treat the second variable as the dependent feature $y$ and compute the linear least squares solution $\beta = A^+y $.</p>
<p class="notes">beta = 28/30, whereas PCA would give 1. explain the difference between the two.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              23/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Exercise</h1></header>
            
            
            <section><p>Given the following data: $(1,2,4), (2,1,3), (3,4,2), (4,3,1)$, treat the third variable as the dependent feature $y$ and compute the linear least squares solution $\beta = A^+y $.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              24/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source:  -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="page_number">
              /38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>We use the pseudoinverse matrix we computed above:
\begin{split}
\beta &amp;= A^+y 
\newline
&amp;= \frac{1}{58} 
\begin{pmatrix}
-13 &amp; 16 &amp; -11 &amp; 18  \\
16 &amp; -13 &amp; 18 &amp; -11  \\
\end{pmatrix} 
\begin{pmatrix}
4 \\
3 \\
2 \\
1 \\
\end{pmatrix} 
\newline
&amp;= \frac{1}{58}
\begin{pmatrix}
-8 \\
50 \\
\end{pmatrix}
\end{split}</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              25/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>$A^+y$ is the orthogonal projection of $y$ onto the m-dimensional subspace spanned by $\mathcal{Im}(A)$. 
$$
$$
This implies that the error $ y - A\beta = y - AA^+y $ is in the kernel of $A^T$, i.e. $A^T(y -A\beta) = 0$.
$$
$$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              26/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Exercise</h1></header>
            
            
            <section><p>Show that for the regression problem above, $ y - A\beta$ is in the kernel of $A^T$.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              27/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Probability Model</h1></header>
            
            
            <section><p>Linear regression is a discriminative regression model of the form:</p>
<p>$$
p(y| x, \beta) = \mathcal{N}( \beta^T x, \sigma^2)
$$
where $\beta$ is an unknown vector and $x, \beta \in \mathbb{R}^n$.
$$
$$
Note that the model can easily be extended to non-linear relationships by transforming the vector $x$ beforehand. It is still linear in $\beta$ and still referred to as linear regression.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              28/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>We fit the unknown coefficient vector $\beta \in \mathbb{R}^m $ by maximizing a log liklifood function. This is a common pattern in machine learning that we will see several times.</p>
<p>\begin{split}
L(\beta) &amp; =  \sum_{i=1}^N \log (p(y_i| x_i, \beta))
\newline
&amp; = \sum_{i=1}^N \log  \left( \frac{1}{\sqrt{2 \pi} \sigma} exp(-\frac{1}{2 \sigma^2}(y_i - \beta^T x_i)^2  )  \right)
\newline
&amp; =  -\frac{N}{2} \log(2 \pi \sigma^2)  -\frac{1}{2 \sigma^2} \sum_{i=1}^N (y_i - \beta^T x_i)^2 
\end{split}</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              29/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>This is equivalent to minimizing the following quantity, known as the residual sum of squares (RSS):</p>
<p>$$
\frac{1}{2} \sum_{i=1}^N (y_i - \beta^T x_i)^2 = \frac{1}{2}  \sum_{i=1}^N \epsilon_i^2 = \frac{1}{2}  \|\epsilon\|^2
$$</p>
<p>The model generally assumes that the 'errors' are i.i.d. (independant and identically distributed).</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              30/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>This is not necessarily an accurate assumption, as the following counter-examples (known as <a href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet">Anscombe's quartet</a>) show:</p>
<p><img alt="" src="img/anscombes-quartet.png" /></p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              31/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>Rewriting the RSS in vector form we get:</p>
<p>$$
\frac{1}{2} (y -  X\beta)^T (y -  X\beta) = \frac{1}{2} \beta^T (X^TX) \beta - \beta^T(X^Ty)
$$</p>
<p>Taking the derivative wrt $\beta$ and setting to zero we again get the normal equations:</p>
<p>\begin{split}
X^TX \beta &amp;= X^T y
\newline
\beta &amp;= (X^TX)^{-1} X^T y
\end{split}
<br>
<br>
If the data vectors $x_i$ are linearly independant then the matrix $X^TX$ will be positive definite and therefore invertible. However in general we will use optimization methods such as Newton-Raphson for calculating $\beta$.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              32/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            <header><h1>Ridge Regression</h1></header>
            
            
            <section><p>Returning to our problem statement $y = Ax$, most real-world phenomena have the effect of low-pass filters in the forward direction. 
$$
$$
Therefore in solving the inverse-problem, $A^+$ operates as a high-pass filter with the undesirable tendency of amplifying noise: the largest singular values in the reverse mapping were the smallest in the forward mapping.
$$
$$
In addition, ordinary least squares implicitly nullifies every element of the reconstructed version of $x$, $\beta$ that is in the kernel of $A$, rather than allowing for a model to be used as a prior for $\beta$. </p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              33/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p><a href="https://en.wikipedia.org/wiki/Tikhonov_regularization">Tikhonov regularization</a> (aka ridge regression) adds to the linear least squares model a second constraint that $\|\beta\|^2$ cannot greater than a given value: 
$$
\min_{\beta \in \mathcal{Im}(X),\|\beta\|^2 \leq c } \|y - \beta\|^2
$$
Equivalently, we may solve an unconstrained minimization of the least-squares penalty with $\lambda\|\beta\|^2$ added, where $\lambda$ is a constant (this is the Lagrange multipliers form of the constrained problem):<br />
$$
\min_{\beta} \|y - X\beta\|^2 + \lambda\|\beta\|^2
$$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              34/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>In a Bayesian context, this is equivalent to placing a zero-mean normally distributed prior distribution on the parameter vector $\beta$:
$$
p(\beta) = \mathcal{N}(0,\tau^2)
$$
where $\frac{1}{\tau^2}$ controls the strength of the prior. 
<br>
<br>
Our probability model then becomes:</p>
<p>$$
p(y, \beta | x) = p(y | x, \beta) p(\beta) =  \mathcal{N}( \beta^T x, \sigma^2) *  \mathcal{N}(0,\tau^2)
$$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              35/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>The corresponding MAP estimation problem is equivalent to the Lagrange multipliers form of the constrained optimization problem above:</p>
<p>\begin{split}
L(\beta) &amp; =  \sum_{i=1}^N \log (p(y_i| x_i, \beta)p(\beta) )
\newline
&amp; =  -\frac{N}{2} \log(2 \pi \sigma^2)  -\frac{1}{2 \sigma^2} \sum_{i=1}^N (y_i - \beta^T x_i)^2   -\frac{1}{2} \log(2 \pi \tau^2) -\frac{1}{2 \tau^2} \sum_{i=2}^n (\beta_i)^2 
\newline
&amp; \propto \|y - X\beta\|^2 + \lambda \|\beta\|^2
\end{split}
where the parameter $\lambda = \frac{\sigma^2}{\tau^2}$.
<br>
<br>
Note that the second sum generally starts at $i=2$ due to the presence of an unpenalized offset term.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              36/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>The solution is a regularized version of the normal equations (hence the name Tikhonov regularization):
$$
\beta = (X^TX + \lambda I)^{-1} X^T y
$$
where $I$ is the $n \times n$ identity matrix. 
$$
$$
These equations are preferable to solve numerically, especially if $X$ is poorly conditioned or rank-deficient (as is the case when several of the features are highly correllated). Note also the connection with the Tikhonov regularization form of the psuedoinverse.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              37/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture4.md -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
            <section><p>Using the SVD of $X$, we have that:
\begin{split}
X\beta &amp;= X(X^TX + \lambda I)^{-1} X^T y
\newline
&amp;= U\Sigma V^T (V\Sigma^2V^T + \lambda I)^{-1} V\Sigma U^T y
\newline
&amp;= U \Sigma (\Sigma^2 + \lambda I)^{-1} \Sigma U^T y
\newline
&amp;= \sum_{i=1}^n \frac{\sigma_i^2}{\sigma_i^2 + \lambda}u_i u_i^T y
\end{split}
Like linear regression, ridge regression expresses $y$ in terms $U$. 
<br>
<br>
It then shrinks these coordinates by the factors $ \frac{\sigma_i^2}{\sigma_i^2 + \lambda}$, penalizing the basis vectors associated with smaller singular values. </p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="source">
              Source: <a href="lecture4.md">lecture4.md</a>
            </aside>
            
            <aside class="page_number">
              38/38
            </aside>

          </footer>
        </div>
      </div>
      
      <!-- slide source:  -->
      <div class="slide-wrapper">
        <div class="slide">
          <div class="inner">
            
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>


          <footer>
            
            <aside class="page_number">
              /38
            </aside>

          </footer>
        </div>
      </div>
      
    </div>
  </div>
  
  <div id="toc" class="sidebar hidden">
    <h2>Table of Contents</h2>
    <table>
      <caption>Table of Contents</caption>
      
      <tr id="toc-row-1">
        <th><a href="#slide1">Lecture 4: Linear Regression</a></th>
        <td><a href="#slide1">1</a></td>
      </tr>
      
      
      <tr id="toc-row-2">
        <th><a href="#slide2">-</a></th>
        <td><a href="#slide2">2</a></td>
      </tr>
      
      
      <tr id="toc-row-3">
        <th><a href="#slide3">Problem</a></th>
        <td><a href="#slide3">3</a></td>
      </tr>
      
      
      <tr id="toc-row-4">
        <th><a href="#slide4">-</a></th>
        <td><a href="#slide4">4</a></td>
      </tr>
      
      
      <tr id="toc-row-5">
        <th><a href="#slide5">Pseudoinverses</a></th>
        <td><a href="#slide5">5</a></td>
      </tr>
      
      
      <tr id="toc-row-6">
        <th><a href="#slide6">-</a></th>
        <td><a href="#slide6">6</a></td>
      </tr>
      
      
      <tr id="toc-row-7">
        <th><a href="#slide7">-</a></th>
        <td><a href="#slide7">7</a></td>
      </tr>
      
      
      <tr id="toc-row-8">
        <th><a href="#slide8">-</a></th>
        <td><a href="#slide8">8</a></td>
      </tr>
      
      
      <tr id="toc-row-9">
        <th><a href="#slide9">Exercise</a></th>
        <td><a href="#slide9">9</a></td>
      </tr>
      
      
      <tr id="toc-row-10">
        <th><a href="#slide10">Properties</a></th>
        <td><a href="#slide10">10</a></td>
      </tr>
      
      
      <tr id="toc-row-11">
        <th><a href="#slide11">-</a></th>
        <td><a href="#slide11">11</a></td>
      </tr>
      
      
      <tr id="toc-row-12">
        <th><a href="#slide12">Exercise</a></th>
        <td><a href="#slide12">12</a></td>
      </tr>
      
      
      <tr id="toc-row-13">
        <th><a href="#slide13">-</a></th>
        <td><a href="#slide13">13</a></td>
      </tr>
      
      
      <tr id="toc-row-14">
        <th><a href="#slide14">-</a></th>
        <td><a href="#slide14">14</a></td>
      </tr>
      
      
      <tr id="toc-row-15">
        <th><a href="#slide15">-</a></th>
        <td><a href="#slide15">15</a></td>
      </tr>
      
      
      <tr id="toc-row-16">
        <th><a href="#slide16">Example</a></th>
        <td><a href="#slide16">16</a></td>
      </tr>
      
      
      <tr id="toc-row-17">
        <th><a href="#slide17">-</a></th>
        <td><a href="#slide17">17</a></td>
      </tr>
      
      
      <tr id="toc-row-18">
        <th><a href="#slide18">-</a></th>
        <td><a href="#slide18">18</a></td>
      </tr>
      
      
      <tr id="toc-row-19">
        <th><a href="#slide19">-</a></th>
        <td><a href="#slide19">19</a></td>
      </tr>
      
      
      <tr id="toc-row-20">
        <th><a href="#slide20">-</a></th>
        <td><a href="#slide20">20</a></td>
      </tr>
      
      
      <tr id="toc-row-21">
        <th><a href="#slide21">Example</a></th>
        <td><a href="#slide21">21</a></td>
      </tr>
      
      
      <tr id="toc-row-22">
        <th><a href="#slide22">-</a></th>
        <td><a href="#slide22">22</a></td>
      </tr>
      
      
      <tr id="toc-row-23">
        <th><a href="#slide23">Exercise</a></th>
        <td><a href="#slide23">23</a></td>
      </tr>
      
      
      <tr id="toc-row-24">
        <th><a href="#slide24">Exercise</a></th>
        <td><a href="#slide24">24</a></td>
      </tr>
      
      
      <tr id="toc-row-25">
        <th><a href="#slide25">-</a></th>
        <td><a href="#slide25">25</a></td>
      </tr>
      
      
      <tr id="toc-row-26">
        <th><a href="#slide26">-</a></th>
        <td><a href="#slide26">26</a></td>
      </tr>
      
      
      <tr id="toc-row-27">
        <th><a href="#slide27">Exercise</a></th>
        <td><a href="#slide27">27</a></td>
      </tr>
      
      
      <tr id="toc-row-28">
        <th><a href="#slide28">Probability Model</a></th>
        <td><a href="#slide28">28</a></td>
      </tr>
      
      
      <tr id="toc-row-29">
        <th><a href="#slide29">-</a></th>
        <td><a href="#slide29">29</a></td>
      </tr>
      
      
      <tr id="toc-row-30">
        <th><a href="#slide30">-</a></th>
        <td><a href="#slide30">30</a></td>
      </tr>
      
      
      <tr id="toc-row-31">
        <th><a href="#slide31">-</a></th>
        <td><a href="#slide31">31</a></td>
      </tr>
      
      
      <tr id="toc-row-32">
        <th><a href="#slide32">-</a></th>
        <td><a href="#slide32">32</a></td>
      </tr>
      
      
      <tr id="toc-row-33">
        <th><a href="#slide33">Ridge Regression</a></th>
        <td><a href="#slide33">33</a></td>
      </tr>
      
      
      <tr id="toc-row-34">
        <th><a href="#slide34">-</a></th>
        <td><a href="#slide34">34</a></td>
      </tr>
      
      
      <tr id="toc-row-35">
        <th><a href="#slide35">-</a></th>
        <td><a href="#slide35">35</a></td>
      </tr>
      
      
      <tr id="toc-row-36">
        <th><a href="#slide36">-</a></th>
        <td><a href="#slide36">36</a></td>
      </tr>
      
      
      <tr id="toc-row-37">
        <th><a href="#slide37">-</a></th>
        <td><a href="#slide37">37</a></td>
      </tr>
      
      
      <tr id="toc-row-38">
        <th><a href="#slide38">-</a></th>
        <td><a href="#slide38">38</a></td>
      </tr>
      
      
    </table>
  </div>
  
  <div id="help" class="sidebar hidden">
    <h2>Help</h2>
    <table>
      <caption>Help</caption>
      <tr>
        <th>Table of Contents</th>
        <td>t</td>
      </tr>
      <tr>
        <th>Exposé</th>
        <td>ESC</td>
      </tr>
      <tr>
        <th>Full screen slides</th>
        <td>e</td>
      </tr>
      <tr>
        <th>Presenter View</th>
        <td>p</td>
      </tr>
      <tr>
        <th>Source Files</th>
        <td>s</td>
      </tr>
      <tr>
        <th>Slide Numbers</th>
        <td>n</td>
      </tr>
      <tr>
        <th>Toggle screen blanking</th>
        <td>b</td>
      </tr>
      <tr>
        <th>Show/hide slide context</th>
        <td>c</td>
      </tr>
      <tr>
        <th>Notes</th>
        <td>2</td>
      </tr>
      <tr>
        <th>Help</th>
        <td>h</td>
      </tr>
    </table>
  </div>
  <script>main()</script>
</body>
</html>