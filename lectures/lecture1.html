<!DOCTYPE html>
<!--
  Copyright 2010 Google Inc.

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

     http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.

  Original slides: Marcin Wichary (mwichary@google.com)
  Modifications: Ernest Delgado (ernestd@google.com)
                 Alex Russell (slightlyoff@chromium.org)

  landslide modifications: Adam Zapletal (adamzap@gmail.com)
                           Nicolas Perriault (nperriault@gmail.com)
-->
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>None</title>
    <!-- Styles -->
    
    <link rel="stylesheet" media="print" href="theme/css/print.css">
    <link rel="stylesheet" media="screen, projection" href="theme/css/screen.css">
    
    
    <!-- /Styles -->
    <!-- Javascripts -->
    
    <script type="text/javascript" src="theme/js/slides.js"></script>
    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX", "output/HTML-CSS"],
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
          processEscapes: true
        },
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script>
    <script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    
    <!-- /Javascripts -->
</head>
<body>
  <div id="blank"></div>
  <div class="presentation">
    <div id="current_presenter_notes">
      <div id="presenter_note"></div>
    </div>
    <div class="slides">
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-1">
          <div class="inner">
            
            
            <section><div style="border-radius: 10px; background: #EEEEEE; padding: 20px; text-align: center; font-size: 1.5em">
  <big><b>Models</b></big> </br>
  </br>

  <code>{amitoj, dami, chris}@datascience.com</code>
  <br/>
  <br/>

</div></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              1/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-2">
          <div class="inner">
            
            <header><h1>Resources</h1></header>
            
            
            <section><ul>
<li>Hastie, Tibshirani and Friedmans's <a href="https://web.stanford.edu/H^castie/local.ftp/Springer/OLD/ESLII_print4.pdf">Elements of Statistical Learning</a></li>
<li>Murphy's <a href="http://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020">Machine Learning: A Probabilistic Approach</a></li>
<li>This course's <a href="https://ds12residency.atlassian.net/wiki/display/MOD/Models">Confluence Page</a></li>
</ul></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              2/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-3">
          <div class="inner">
            
            <header><h1>Lecture 1: Learning</h1></header>
            
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              3/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-4">
          <div class="inner">
            
            
            <section><p><img alt="" src="img/tukey.jpg" /></p>
<p>"Data analysis is a very difficult field. It must adapt itself to what people can and
need to do with data. In the sense that biology is more complex than physics, and the
behavioral sciences are more complex than either, it is likely that the general problems
of data analysis are more complex than those of all three."</p>
<ul>
<li>John Tukey and M.B. Wilk, <a href="https://www.computer.org/csdl/proceedings/afips/1966/5068/00/50680695.pdf">Data Analysis and Statistics, An Exploratory Overview</a>, 1966</li>
</ul></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              4/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-5">
          <div class="inner">
            
            
            <section><p>In the above paper Tukey and Wilk identified four major influences acting on data analysis:</p>
<ol>
<li>The formal theories of statistics</li>
<li>Accelerating developments in computers and display devices</li>
<li>The challenge, in many fields, of more and ever larger bodies of data</li>
<li>The emphasis on quantification in an ever wider variety of disciplines</li>
</ol>
<p>Tukey and Wilk's list is surprisingly modern. However in 1966 their viewpoint was practically heretical within the statistics community.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              5/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-6">
          <div class="inner">
            
            
            <section><p>In a 2001 paper entitled <a href="http://projecteuclid.org/euclid.ss/1009213726">‘Statistical Modeling: The Two Cultures’</a>, Leo Breiman described two cultural outlooks about extracting information from data.
$$
$$</p>
<blockquote>
<p>Statistics starts with data. Think of the data as being generated by a black box in which a vector of input variables x (independent variables) go in one side, and on the other side the response variables y come out. Inside the black box, nature functions to associate the predictor variables with the response variables.</p>
</blockquote></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              6/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-7">
          <div class="inner">
            
            
            <section><p>Breiman says that users of data split into two cultures, based on their primary allegiance to one or the other of the following goals:</p>
<ul>
<li>
<p>Inference: to infer how nature is associating the response variables to the input variables.</p>
</li>
<li>
<p>Prediction: to be able to predict what the responses are going to be to future input variables.</p>
</li>
</ul>
<p>These two cultures though about statistical modeling in different ways.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              7/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-8">
          <div class="inner">
            
            <header><h1>Generative modeling:</h1></header>
            
            
            <section><p>One proposes a stochastic model that could have generated the data, and derives methods to infer properties of the underlying generative mechanism (e.g. as in linear discriminant analysis). 
$$
$$
This roughly speaking coincides with traditional academic statistics and its offshoots. 
$$
$$
Implicit in this viewpoint is the notion that there is a true model generating the data, and often a truly ‘best’ way to analyze the data .</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              8/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-9">
          <div class="inner">
            
            <header><h1>Predictive modeling</h1></header>
            
            
            <section><p>One constructs methods which predict well over some some given data universe – i.e. some very specific concrete dataset. 
$$
$$
This roughly coincides with modern machine Learning, and its industrial offshoots. 
$$
$$
Predictive modeling (e.g. as in logistic regression) is effectively silent about the underlying mechanism generating the data, and allows for many different predictive algorithms, preferring to discuss only accuracy of prediction.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              9/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-10">
          <div class="inner">
            
            
            <section><p>Another interesting manifestation of this trend was the <a href="http://www.tor.com/blogs/2011/06/norvig-vs-chomsky-and-the-fight-for-the-future-of-ai">public argument</a> between Noam Chomsky and Peter Norvig on the nature of language. 
$$
$$
Chomsky long ago proposed a hierarchical model of formal language grammars. Peter Norvig is a proponent of probabilistic models of language. 
$$
$$
Indeed all successful automated language processing systems are probabilistic.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              10/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-11">
          <div class="inner">
            
            <header><h1>Supervised Learning</h1></header>
            
            
            <section><ul>
<li>unknown target function (the black box) $f: \mathcal{X} \to \mathcal{Y}$</li>
<li>training examples $(x_1,y_1), \dots, (x_N, y_N)$</li>
<li>set of hypothesis functions (the model) $\mathcal{H}$</li>
<li>training algorithm $\mathcal{A}$ (optimization problem plus optimizer).</li>
<li>final hypothesis $h \in \mathcal{H}, h \sim f$.</li>
</ul>
<p>Note that models and training algorithms are largely independent. For example, linear regression could be trained by the normal equations or by gradient descent, decision trees could be trained by entropy or gini impurity, etc.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              11/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-12">
          <div class="inner">
            
            <header><h1>Challenge Question</h1></header>
            
            
            <section><p>In the following binary classification example, let $ \mathcal{X} = \{0,1\}^3$. </p>
<p><center> 
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;border-color:#ccc;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;}
.tg .tg-9hbo{font-weight:bold;vertical-align:top}
.tg .tg-yw4l{vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-9hbo">$x_i$</th>
    <th class="tg-9hbo">$y$</th>
  </tr>
  <tr>
    <td class="tg-yw4l">(0,0,0)</td>
    <td class="tg-9hbo">0</td>
  </tr>
  <tr>
    <td class="tg-yw4l">(0,0,1)</td>
    <td class="tg-9hbo">1</td>
  </tr>
  <tr>
    <td class="tg-yw4l">(0,1,0)</td>
    <td class="tg-9hbo">1</td>
  </tr>
  <tr>
    <td class="tg-yw4l">(0,1,1)</td>
    <td class="tg-9hbo">0</td>
  </tr>
  <tr>
    <td class="tg-yw4l">(1,0,0)</td>
    <td class="tg-yw4l">1</td>
  </tr>
</table>
</center></p>
<p>How large is $\mathcal{H} = \{ h | h: \mathcal{X} \to \{0,1\} \}$?
$$
$$
How many functions $h \in \mathcal{H}$ agree with $f$ on the data?</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              12/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source:  -->
      <div class="slide-wrapper">
        <div class="slide slide-">
          <div class="inner">
            
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="page_number">
              /37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-13">
          <div class="inner">
            
            
            <section><p><center> 
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;border-color:#ccc;}
.tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#fff;}
.tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#ccc;color:#333;background-color:#f0f0f0;}
.tg .tg-9hbo{font-weight:bold;vertical-align:top}
.tg .tg-yw4l{vertical-align:top}
</style>
<table class="tg">
  <tr>
    <th class="tg-9hbo">$x_i$</th>
    <th class="tg-9hbo">$y$</th>
    <th class="tg-9hbo">$h_0(x_i)$</th>
    <th class="tg-9hbo">$h_1(x_i)$</th>
    <th class="tg-9hbo">$h_2(x_i)$</th>
    <th class="tg-9hbo">$h_3(x_i)$</th>
    <th class="tg-9hbo">$h_4(x_i)$</th>
    <th class="tg-9hbo">$h_5(x_i)$</th>
    <th class="tg-9hbo">$h_6(x_i)$</th>
    <th class="tg-9hbo">$h_7(x_i)$</th>
  </tr>
  <tr>
    <td class="tg-yw4l">(0,0,0)</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
  </tr>
  <tr>
    <td class="tg-yw4l">(0,0,1)</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
  </tr>
  <tr>
    <td class="tg-yw4l">(0,1,0)</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
  </tr>
  <tr>
    <td class="tg-yw4l">(0,1,1)</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
  </tr>
  <tr>
    <td class="tg-yw4l">(1,0,0)</td>
    <td class="tg-yw4l">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
  </tr>
  <tr>
    <td class="tg-yw4l">(1,0,1)</td>
    <td class="tg-yw4l">?</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
  </tr>
  <tr>
    <td class="tg-yw4l">(1,1,0)</td>
    <td class="tg-yw4l">?</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">1</td>
  </tr>
  <tr>
    <td class="tg-yw4l">(1,1,1)</td>
    <td class="tg-yw4l">?</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">1</td>
    <td class="tg-9hbo">0</td>
    <td class="tg-9hbo">1</td>
  </tr>
</table>
</center></p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              13/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-14">
          <div class="inner">
            
            
            <section><p>You cannot know anything for sure about $f$ outside the data without making assumptions. This fact is often referred to as the <a href="http://www.no-free-lunch.org/Wolp01a.pdf">No Free Lunch Theorem</a>, which states that there is no one model that works best for every problem. 
$$
$$
The assumptions of a great model for one problem may not hold for another problem, so it is common in machine learning to try multiple models and find one that works best.
$$
$$<br />
It is also important to assess the implementation trade-offs between different models from the application's perspective.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              14/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-15">
          <div class="inner">
            
            <header><h1>Many other tradeoffs in ML</h1></header>
            
            
            <section><ul>
<li>Frequentist vs Bayesian</li>
<li>Supervised vs Unsupervised</li>
<li>Classification vs Regression</li>
<li><a href="http://stackoverflow.com/questions/879432/what-is-the-difference-between-a-generative-and-discriminative-algorithm">Generative vs Discriminative</a></li>
<li><a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">Bias vs Variance</a></li>
<li><a href="https://www.quora.com/What-is-the-difference-between-the-parametric-model-and-the-non-parametric-model">Parametric vs non-parametric</a></li>
<li><a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">Low dimensions vs high dimensions</a></li>
</ul></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              15/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-16">
          <div class="inner">
            
            <header><h1>Feasibility of learning</h1></header>
            
            
            <section><p>Is there any hope of learning anything about $f$ outside the data set without making assumptions about $f$?
$$
$$
Yes, if we are willing to give up deterministic guarantees.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              16/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-17">
          <div class="inner">
            
            
            <section><p>Consider an urn with black and white marbles.</p>
<p>\begin{align}
P(black) &amp;= \mu
\newline
P(white)  &amp;= 1 − \mu
\end{align}</p>
<p>Suppose the value of $\mu$ is unknown to us. Suppose also that we pick $n$ marbles independently and the fraction of black marbles in our sample is $\nu_n$. 
<br>
<br>
Does $\nu_n$ say anything about $\mu$?</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              17/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-18">
          <div class="inner">
            
            <header><h1>Hoeffding's Inequality</h1></header>
            
            
            <section><p>In a big sample (large $n$), $\nu$ is probably close to $\mu$:
$$
P(|\nu − \mu| &gt; \epsilon) \leq 2 e^{-2 \epsilon^2 n}
$$</p>
<p><a href="https://en.wikipedia.org/wiki/Hoeffding%27s_inequality">Hoeffding's Inequality</a> says that $\nu_n$ converges to $\mu$ <a href="https://en.wikipedia.org/wiki/Convergence_in_measure">in measure</a>, i.e. 
$$
\lim_{n\to\infty}P\big(|\nu_n-\mu| \geq \epsilon \big) = 0
$$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              18/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-19">
          <div class="inner">
            
            <header><h1>Example</h1></header>
            
            
            <section><p>Let $n = 1000$; draw a sample and observe $\nu$. 98.7% of the time we will find that</p>
<p>$$
\mu -0.05 \leq \nu \leq \mu + 0.05
$$
since 
$$
P(|\nu − \mu| &gt; 0.05) \leq 2 e^{-2000 0.05^2 } = 0.0134
$$
So on any particular sample we might be wrong, but it is unlikely.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              19/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-20">
          <div class="inner">
            
            <header><h2>Key ingredients</h2></header>
            
            
            <section><ul>
<li>
<p>Samples must be independent. If the sample is constructed in some arbitrary fashion, then indeed we cannot say anything.</p>
</li>
<li>
<p>Even with independence, $\nu$ can take on arbitrary values; but some values are way more likely than others.</p>
</li>
<li>
<p>The bound fortunately does not depend on $\mu$ or the size of the urn. The urn can be infinite.</p>
</li>
<li>
<p>The key terms in the bound are $\epsilon$ and $n$. </p>
</li>
</ul></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              20/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-21">
          <div class="inner">
            
            <header><h1>Connetion to learning</h1></header>
            
            
            <section><p>In supervised learning, the unknown is a function $f$. Each marble is a point $x \in \mathcal{X}$: </p>
<ul>
<li>black marble: $h(x)=f(x)$, i.e. hypothesis got it right</li>
<li>white marble: $h(x) \neq f(x)$, i.e. hypothesis got it wrong</li>
</ul></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              21/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-22">
          <div class="inner">
            
            
            <section><p>For a fixed $h \in \mathcal{H}$ Hoeffding's inequality states that 
$$
P(|Err_{in}(h) − Err_{out}(h) | &gt; \epsilon) \leq 2 e^{-2 \epsilon^2 n}
$$</p>
<p>i.e. the <em>in-sample error</em> $Err_{in}(h)$ is probably approximately equal to the <em>out-of-sample error</em> $Err_{out}(h)$. 
<br>
<br>
Thus if $Err_{in}(h)$ is small then we have managed to learn something about $f$.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              22/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-23">
          <div class="inner">
            
            <header><h1>Challenge Question</h1></header>
            
            
            <section><p>Does this application of Hoeffding's inequality violate the No Free Lunch theorem?</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              23/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source:  -->
      <div class="slide-wrapper">
        <div class="slide slide-">
          <div class="inner">
            
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="page_number">
              /37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-24">
          <div class="inner">
            
            
            <section><p>No, because we've implicitly assumed that $h$ is fixed, i.e. $\mathcal{H} = h$, and in this case the <em>in-sample error</em> $Err_{in}(h)$ is probably quite large.
$$
$$
However in our supervised learning framework $\mathcal{A}$ must select an $h$ from a larger hypothesis space.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              24/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-25">
          <div class="inner">
            
            <header><h1>Multiple bins</h1></header>
            
            
            <section><p>Let $\mathcal{H} = \{h_1, \dots, h_m\}$.
$$
$$
This means we have multiple urns, one for each hypothesis function $h_k$:
$$
P(black) = P(h_k(x) = f(x)) = \mu_k
$$
We then pick $n$ marbles independently from each urn and define the fration of black marbles in each sample to be $\nu_k$.
$$
$$
We could then pick the $h_k$ with the largest $\nu_k$.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              25/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide has_notes slide-26">
          <div class="inner">
            
            <header><h1>Challenge Question</h1></header>
            
            
            <section><p>If you toss a fair coin 10 times, what is the probability that you will get 10 heads?
$$
$$
If you toss 1000 fair coins 10 times each, what is the probability that some coin will get 10 heads?</p>
<p class="notes">0.5^10 ~ 0.001 and 1-(1-0.5^10)^1000 ~ 0.62. We will see this same computation when we discuss locality sensitive hashing.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              26/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-27">
          <div class="inner">
            
            
            <section><p><img src="img/xkcd-bias.png" height="420" width="620"></p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              27/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-28">
          <div class="inner">
            
            <header><h1>Bias and Variance</h1></header>
            
            
            <section><p>So the $h_k$ with the largest $\nu_k$ may just be <a href="https://en.wikipedia.org/wiki/Survivorship_bias">lucky</a>.
$$
$$
$Err_{in}$ can be a poor estimate of $Err_{out}$—and it can get worse when $\mathcal{H}$ is large.
$$
$$
This is known as the <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">bias-variance tradeoff</a>.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              28/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-29">
          <div class="inner">
            
            
            <section><p>For a bound on the out-of-sample error on some undetermined $\widetilde{h} \in \mathcal{H}$, we use a union bound along with Hoeffding's inequality:
\begin{align}
P(|Err_{in}(\widetilde{h}) − Err_{out}(\widetilde{h}) | &gt; \epsilon) &amp;\leq \sum_{k=1}^m P(|Err_{in}(h_{k}) − Err_{out}(h_{k}) | &gt; \epsilon) 
\newline
&amp;\leq \sum_{k=1}^m  2 e^{-2 \epsilon^2 n}
\newline
&amp;\leq  2m e^{-2 \epsilon^2 n}
\newline
&amp;\leq  2|\mathcal{H}| e^{-2 \epsilon^2 n}
\end{align}
Note that when $\mathcal{H}$ is infinite we shall use the concept of <a href="https://en.wikipedia.org/wiki/VC_dimension">VC dimension</a>. </p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              29/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-30">
          <div class="inner">
            
            
            <section><p><strong>Corollary:</strong>  If $ \widetilde{h} \in \mathcal{H}$, then with probability at least $1 - \delta$,
$$
Err_{out}(\widetilde{h}) \leq Err_{in}(\widetilde{h}) + \sqrt{\frac{1}{2n}\ln(\frac{2|\mathcal{H}|}{\delta})}
$$</p>
<p>Thus $Err_{out} \sim Err_{in}$ when $n &gt;&gt; \ln(|\mathcal{H}|)$. </p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              30/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-31">
          <div class="inner">
            
            
            <section><p><strong>Proof:</strong>  Let $\delta =  2|\mathcal{H}| e^{-2 \epsilon^2 n}$. Then $P(|Err_{in}(\widetilde{h}) − Err_{out}(\widetilde{h}) | \leq \epsilon) \geq 1 - \delta$. 
$$
$$
This implies that with probability at least $1-\delta$:
\begin{align}
|Err_{in}(\widetilde{h}) − Err_{out}(\widetilde{h}) | &amp;\leq \epsilon
\newline
Err_{out}(\widetilde{h}) &amp;\leq Err_{in}(\widetilde{h}) + \epsilon
\newline
&amp;\leq Err_{in}(\widetilde{h}) +   \sqrt{\frac{1}{2n}\ln(\frac{2|\mathcal{H}|}{\delta})}
\end{align}
by the definition of $\epsilon$.</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              31/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-32">
          <div class="inner">
            
            
            <section><p><img alt="" src="img/learning.png" /></p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              32/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-33">
          <div class="inner">
            
            <header><h1>Caveats</h1></header>
            
            
            <section><p>Simple $f \implies$ small $|\mathcal{H}|$ is sufficient for a small $Err_{out}$
<br>
<br>
Complex $f \implies$ large $|\mathcal{H}|$ is necessary for a small $Err_{out}$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              33/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-34">
          <div class="inner">
            
            
            <section><p>Small $|\mathcal{H}| \implies Err_{out} \sim Err_{in} $ 
<br>
<br>
Large $|\mathcal{H}| \implies Err_{in} \sim 0 $ </p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              34/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-35">
          <div class="inner">
            
            <header><h1>2 Step Approach to Learning</h1></header>
            
            
            <section><p>Step 1: Confirm that $Err_{out} \sim Err_{in}$ 
$$
$$
Step 2: Attempt to decrease $Err_{in}$</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              35/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-36">
          <div class="inner">
            
            <header><h1>Challenge Question</h1></header>
            
            
            <section><p>Order the three datasets in terms of evidence provided for the hypothesis that $\rho$ is linear in $T$.</p>
<p><img src="img/3-lines.png" height="180" width="700"></p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              36/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source:  -->
      <div class="slide-wrapper">
        <div class="slide slide-">
          <div class="inner">
            
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="page_number">
              /37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source: lecture1.md -->
      <div class="slide-wrapper">
        <div class="slide slide-37">
          <div class="inner">
            
            <header><h1>Non-Falsifiability</h1></header>
            
            
            <section><p>If an experiment has no chance of falsifying a hypothesis, then the result of that experiment provides no evidence one way or the other for the hypothesis.
$$
$$
A good fit is surprising with simpler $\mathcal{H}$ and hence more significant</p></section>
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="source">
              Source: <a href="lecture1.md">lecture1.md</a>
            </aside>
            
            <aside class="page_number">
              37/37
            </aside>
          </footer>
        </div>
      </div>
      
      <!-- slide source:  -->
      <div class="slide-wrapper">
        <div class="slide slide-">
          <div class="inner">
            
            
          </div>
          <div class="presenter_notes">
            <header><h1>Presenter Notes</h1></header>
            <section>
            
            </section>
          </div>
          <footer>
            
            <aside class="page_number">
              /37
            </aside>
          </footer>
        </div>
      </div>
      
    </div>
  </div>
  
  <div id="toc" class="sidebar hidden">
    <h2>Table of Contents</h2>
    <table>
      <caption>Table of Contents</caption>
      
      <tr id="toc-row-1">
        <th><a href="#slide1">-</a></th>
        <td><a href="#slide1">1</a></td>
      </tr>
      
      
      <tr id="toc-row-2">
        <th><a href="#slide2">Resources</a></th>
        <td><a href="#slide2">2</a></td>
      </tr>
      
      
      <tr id="toc-row-3">
        <th><a href="#slide3">Lecture 1: Learning</a></th>
        <td><a href="#slide3">3</a></td>
      </tr>
      
      
      <tr id="toc-row-4">
        <th><a href="#slide4">-</a></th>
        <td><a href="#slide4">4</a></td>
      </tr>
      
      
      <tr id="toc-row-5">
        <th><a href="#slide5">-</a></th>
        <td><a href="#slide5">5</a></td>
      </tr>
      
      
      <tr id="toc-row-6">
        <th><a href="#slide6">-</a></th>
        <td><a href="#slide6">6</a></td>
      </tr>
      
      
      <tr id="toc-row-7">
        <th><a href="#slide7">-</a></th>
        <td><a href="#slide7">7</a></td>
      </tr>
      
      
      <tr id="toc-row-8">
        <th><a href="#slide8">Generative modeling:</a></th>
        <td><a href="#slide8">8</a></td>
      </tr>
      
      
      <tr id="toc-row-9">
        <th><a href="#slide9">Predictive modeling</a></th>
        <td><a href="#slide9">9</a></td>
      </tr>
      
      
      <tr id="toc-row-10">
        <th><a href="#slide10">-</a></th>
        <td><a href="#slide10">10</a></td>
      </tr>
      
      
      <tr id="toc-row-11">
        <th><a href="#slide11">Supervised Learning</a></th>
        <td><a href="#slide11">11</a></td>
      </tr>
      
      
      <tr id="toc-row-12">
        <th><a href="#slide12">Challenge Question</a></th>
        <td><a href="#slide12">12</a></td>
      </tr>
      
      
      <tr id="toc-row-13">
        <th><a href="#slide13">-</a></th>
        <td><a href="#slide13">13</a></td>
      </tr>
      
      
      <tr id="toc-row-14">
        <th><a href="#slide14">-</a></th>
        <td><a href="#slide14">14</a></td>
      </tr>
      
      
      <tr id="toc-row-15">
        <th><a href="#slide15">Many other tradeoffs in ML</a></th>
        <td><a href="#slide15">15</a></td>
      </tr>
      
      
      <tr id="toc-row-16">
        <th><a href="#slide16">Feasibility of learning</a></th>
        <td><a href="#slide16">16</a></td>
      </tr>
      
      
      <tr id="toc-row-17">
        <th><a href="#slide17">-</a></th>
        <td><a href="#slide17">17</a></td>
      </tr>
      
      
      <tr id="toc-row-18">
        <th><a href="#slide18">Hoeffding's Inequality</a></th>
        <td><a href="#slide18">18</a></td>
      </tr>
      
      
      <tr id="toc-row-19">
        <th><a href="#slide19">Example</a></th>
        <td><a href="#slide19">19</a></td>
      </tr>
      
        
        <tr id="toc-row-20" class="sub">
          <th><a href="#slide20">Key ingredients</a></th>
          <td><a href="#slide20">20</a></td>
        </tr>
        
      
      
      <tr id="toc-row-21">
        <th><a href="#slide21">Connetion to learning</a></th>
        <td><a href="#slide21">21</a></td>
      </tr>
      
      
      <tr id="toc-row-22">
        <th><a href="#slide22">-</a></th>
        <td><a href="#slide22">22</a></td>
      </tr>
      
      
      <tr id="toc-row-23">
        <th><a href="#slide23">Challenge Question</a></th>
        <td><a href="#slide23">23</a></td>
      </tr>
      
      
      <tr id="toc-row-24">
        <th><a href="#slide24">-</a></th>
        <td><a href="#slide24">24</a></td>
      </tr>
      
      
      <tr id="toc-row-25">
        <th><a href="#slide25">Multiple bins</a></th>
        <td><a href="#slide25">25</a></td>
      </tr>
      
      
      <tr id="toc-row-26">
        <th><a href="#slide26">Challenge Question</a></th>
        <td><a href="#slide26">26</a></td>
      </tr>
      
      
      <tr id="toc-row-27">
        <th><a href="#slide27">-</a></th>
        <td><a href="#slide27">27</a></td>
      </tr>
      
      
      <tr id="toc-row-28">
        <th><a href="#slide28">Bias and Variance</a></th>
        <td><a href="#slide28">28</a></td>
      </tr>
      
      
      <tr id="toc-row-29">
        <th><a href="#slide29">-</a></th>
        <td><a href="#slide29">29</a></td>
      </tr>
      
      
      <tr id="toc-row-30">
        <th><a href="#slide30">-</a></th>
        <td><a href="#slide30">30</a></td>
      </tr>
      
      
      <tr id="toc-row-31">
        <th><a href="#slide31">-</a></th>
        <td><a href="#slide31">31</a></td>
      </tr>
      
      
      <tr id="toc-row-32">
        <th><a href="#slide32">-</a></th>
        <td><a href="#slide32">32</a></td>
      </tr>
      
      
      <tr id="toc-row-33">
        <th><a href="#slide33">Caveats</a></th>
        <td><a href="#slide33">33</a></td>
      </tr>
      
      
      <tr id="toc-row-34">
        <th><a href="#slide34">-</a></th>
        <td><a href="#slide34">34</a></td>
      </tr>
      
      
      <tr id="toc-row-35">
        <th><a href="#slide35">2 Step Approach to Learning</a></th>
        <td><a href="#slide35">35</a></td>
      </tr>
      
      
      <tr id="toc-row-36">
        <th><a href="#slide36">Challenge Question</a></th>
        <td><a href="#slide36">36</a></td>
      </tr>
      
      
      <tr id="toc-row-37">
        <th><a href="#slide37">Non-Falsifiability</a></th>
        <td><a href="#slide37">37</a></td>
      </tr>
      
      
    </table>
  </div>
  
  <div id="help" class="sidebar hidden">
    <h2>Help</h2>
    <table>
      <caption>Help</caption>
      <tr>
        <th>Table of Contents</th>
        <td>t</td>
      </tr>
      <tr>
        <th>Exposé</th>
        <td>ESC</td>
      </tr>
      <tr>
        <th>Full screen slides</th>
        <td>e</td>
      </tr>
      <tr>
        <th>Presenter View</th>
        <td>p</td>
      </tr>
      <tr>
        <th>Source Files</th>
        <td>s</td>
      </tr>
      <tr>
        <th>Slide Numbers</th>
        <td>n</td>
      </tr>
      <tr>
        <th>Toggle screen blanking</th>
        <td>b</td>
      </tr>
      <tr>
        <th>Show/hide slide context</th>
        <td>c</td>
      </tr>
      <tr>
        <th>Notes</th>
        <td>2</td>
      </tr>
      <tr>
        <th>Help</th>
        <td>h</td>
      </tr>
    </table>
  </div>
  <script>main()</script>
</body>
</html>
